{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26359bdf-00a9-4280-9ec4-6ca7295148e4",
    "_uuid": "108609d6b29e75ec9f0fe15f66b4336f694c74b2"
   },
   "source": [
    "## VGG implementation with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4eb5ad5-03ba-4fc4-b417-451e30a01fd1",
    "_uuid": "be3002e45fc687fc33e7c7ae9d869ac7ee926b1a"
   },
   "source": [
    "*Python Modules*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "0e05b5f8-fc51-404d-a9d2-5197aa283b73",
    "_uuid": "76fd0ec2a5eb7fbe49b51147eabbd109c61279c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sklearn.svm\n",
    "\n",
    "plt.ion() \n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e9d7ef88-fdbd-4b73-b64f-70294976d238",
    "_uuid": "83371f19c7f1e261bb0f9cc71f7a265c37a4c16a",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1525007461873,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "xvsy0IR4wheJ",
    "outputId": "bb02efaa-518c-4342-d6e5-7275a7d7fdd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7500 images under train\n",
      "Loaded 2500 images under test\n",
      "Classes: \n",
      "['beef_tartare', 'caesar_salad', 'chocolate_cake', 'croque_madame', 'escargots', 'fried_calamari', 'macaroni_and_cheese', 'poutine', 'spring_rolls', 'tuna_tartare']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"C:/Users/Umashanker Deekshith/Google Drive/Germany/Uni-Bonn/Semester 3/Deep Learning for VR/Exercise/DeepLearningWS/project/Deep-Learning-Project/src/images\"\n",
    "TRAIN = 'train'\n",
    "# VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally. \n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "#     VAL: transforms.Compose([\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#     ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=1,\n",
    "        shuffle=True, num_workers=1\n",
    "    )\n",
    "    for x in [TRAIN, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, TEST]}\n",
    "\n",
    "for x in [TRAIN, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ad1e66d-dcfc-429a-9664-e61b4d2944cc",
    "_uuid": "80417b6d4f2000be40245a090424b2452e1b78e1",
    "colab_type": "text",
    "id": "6f1fb14iwheO"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Some utility function to visualize the dataset and the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "fd84399d-83f9-4af7-8767-fe1d32856493",
    "_uuid": "48e7f7c02cab559638af532e98d371ebd8c89bfa",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1525007467056,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "rphPgOQewheQ",
    "outputId": "0bec4c14-9968-4119-bb75-f896ddc21ebd"
   },
   "outputs": [],
   "source": [
    "# def imshow(inp, title=None):\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     # plt.figure(figsize=(10, 10))\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)\n",
    "\n",
    "# def show_databatch(inputs, classes):\n",
    "#     out = torchvision.utils.make_grid(inputs)\n",
    "#     imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders[TRAIN]))\n",
    "# show_databatch(inputs, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5e6e0c3f-19bd-4034-b408-eeffb9746275",
    "_uuid": "d7785c091b61ab0ddff8556c06bafc5f16037aa4",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WXgC3SYkwheT"
   },
   "outputs": [],
   "source": [
    "# def visualize_model(vgg, num_images=6):\n",
    "#     was_training = vgg.training\n",
    "    \n",
    "#     # Set model for evaluation\n",
    "#     vgg.train(False)\n",
    "#     vgg.eval() \n",
    "    \n",
    "#     images_so_far = 0\n",
    "\n",
    "#     for i, data in enumerate(dataloaders[TEST]):\n",
    "#         inputs, labels = data\n",
    "#         size = inputs.size()[0]\n",
    "        \n",
    "#         if use_gpu:\n",
    "#             inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "#         else:\n",
    "#             inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "        \n",
    "#         outputs = vgg(inputs)\n",
    "        \n",
    "#         _, preds = torch.max(outputs.data, 1)\n",
    "#         predicted_labels = [preds[j] for j in range(inputs.size()[0])]\n",
    "        \n",
    "#         print(\"Ground truth:\")\n",
    "#         show_databatch(inputs.data.cpu(), labels.data.cpu())\n",
    "#         print(\"Prediction:\")\n",
    "#         show_databatch(inputs.data.cpu(), predicted_labels)\n",
    "        \n",
    "#         del inputs, labels, outputs, preds, predicted_labels\n",
    "#         torch.cuda.empty_cache()\n",
    "        \n",
    "#         images_so_far += size\n",
    "#         if images_so_far >= num_images:\n",
    "#             break\n",
    "        \n",
    "#     vgg.train(mode=was_training) # Revert model back to original training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1b74302a-9418-4472-970f-76591d00cecb",
    "_uuid": "6d1b1cd49e177152940d8f667b9fa5e2e1c5e360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(vgg, criterion):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "    \n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    print(\"Evaluating model\")\n",
    "    print('-' * 10)\n",
    "    \n",
    "    for i, data in enumerate(dataloaders[TEST]):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "\n",
    "        vgg.train(False)\n",
    "        vgg.eval()\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss_test += loss.data[0]\n",
    "        acc_test += torch.sum(preds == labels.data)\n",
    "\n",
    "        del inputs, labels, outputs, preds\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "    \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "4be764f7-24ff-4611-8fc6-31b87e3ed171",
    "_uuid": "480b7181f00142865d3e971c799dd42bc9d1f7e5",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1003
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2623,
     "status": "ok",
     "timestamp": 1525007474565,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "SjHLMTldwheY",
    "outputId": "4c40caae-9d25-47e5-fe17-a4f3f944487e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model from pytorch\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.load_state_dict(torch.load(\"../input/vgg16bn/vgg16_bn.pth\"))\n",
    "# print(vgg16.classifier[6].out_features) # 1000 \n",
    "\n",
    "# Freeze training for all layers\n",
    "for param in vgg16.features.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# Newly created modules have require_grad=True by default\n",
    "# num_features = vgg16.classifier[6].in_features\n",
    "features = list(vgg16.classifier.children())[:-5] # Remove last layer\n",
    "# features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n",
    "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "731d8833-c6d1-4cba-996c-882ccf547f95",
    "_uuid": "cc4ce91edbd602335e92505a269de62e077385ec"
   },
   "source": [
    "The output above is the summary of our model. Notice how the last layer has 4 output features as we specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f45bda67-096e-4de9-a801-98742c34207c",
    "_uuid": "bd59ea5c500e55b3ae4bd9cdfeae01bd50818668",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Xuaq38UOwhec"
   },
   "outputs": [],
   "source": [
    "# If you want to train the model for more than 2 epochs, set this to True after the first run\n",
    "resume_training = False\n",
    "\n",
    "if resume_training:\n",
    "    print(\"Loading pretrained model..\")\n",
    "    vgg16.load_state_dict(torch.load('../input/vgg16-transfer-learning-pytorch/VGG16_v2-OCT_Retina.pt'))\n",
    "    print(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "df23921b-f26e-496a-9a71-cdf2a9daa34e",
    "_uuid": "45d872d00fcfe93de8938b8741b4526af971c62b",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HTJWo25nwhef"
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    vgg16.cuda() #.cuda() will move everything to the GPU side\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "f7aec745-2087-4f60-987d-a34a995fd6a9",
    "_uuid": "7732406aae91a82348fcb830a5ff5785d22526fb",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1986,
     "status": "ok",
     "timestamp": 1525007488210,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "jSa-X3XVwheo",
    "outputId": "d6f338d4-5379-4fe8-aaec-cc8141b7cbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test before training\n"
     ]
    }
   ],
   "source": [
    "print(\"Test before training\")\n",
    "# eval_model(vgg16, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "dabed264-9a90-4888-ad3c-65924ad9c80d",
    "_uuid": "eff4aa2d97fd5443195ae2a2cd43c5f73e6775a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize_model(vgg16) #test before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8f6936eb-ae6f-44ee-90a9-c7f817ba6eda",
    "_uuid": "abe5dc35b31e7971e7d63637866132f89e7d011d",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lHkiBU5fwhet"
   },
   "outputs": [],
   "source": [
    "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss_val = 0\n",
    "    avg_acc_val = 0\n",
    "    \n",
    "    train_batches = len(dataloaders[TRAIN])\n",
    "    val_batches = len(dataloaders[VAL])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        \n",
    "        vgg.train(True)\n",
    "        \n",
    "        for i, data in enumerate(dataloaders[TRAIN]):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "                \n",
    "            # Use half training dataset\n",
    "            if i >= train_batches / 2:\n",
    "                break\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = vgg(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.data[0]\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print()\n",
    "        # * 2 as we only used half of the dataset\n",
    "        avg_loss = loss_train * 2 / dataset_sizes[TRAIN]\n",
    "        avg_acc = acc_train * 2 / dataset_sizes[TRAIN]\n",
    "        \n",
    "        vgg.train(False)\n",
    "        vgg.eval()\n",
    "            \n",
    "        for i, data in enumerate(dataloaders[VAL]):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
    "                \n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = vgg(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_val += loss.data[0]\n",
    "            acc_val += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss_val = loss_val / dataset_sizes[VAL]\n",
    "        avg_acc_val = acc_val / dataset_sizes[VAL]\n",
    "        \n",
    "        print()\n",
    "        print(\"Epoch {} result: \".format(epoch))\n",
    "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
    "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
    "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
    "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
    "        print('-' * 10)\n",
    "        print()\n",
    "        \n",
    "        if avg_acc_val > best_acc:\n",
    "            best_acc = avg_acc_val\n",
    "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    vgg.load_state_dict(best_model_wts)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(vgg, num_epochs=10):\n",
    "    train_batches = 100\n",
    "    imgfeatures = []\n",
    "\n",
    "    for i, data in enumerate(dataloaders[TRAIN]):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "\n",
    "        # Use half training dataset\n",
    "        if i >= train_batches / 2:\n",
    "            break\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        feature = vgg16(inputs)\n",
    "        imgfeatures.append(feature.flatten())\n",
    "    return imgfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "53eeb478-e106-49be-9682-0173e76640f8",
    "_uuid": "a0ddf54b1c45b7c61724cbe1e74ca0628f8b1d8e",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1525023518627,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "r1-I_IUUwhew",
    "outputId": "b764e48a-00fd-4eb5-f592-f4386a875ed1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_features() missing 1 required positional argument: 'vgg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d181e8dfacc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimgfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_features() missing 1 required positional argument: 'vgg'"
     ]
    }
   ],
   "source": [
    "imgfeatures = get_features(vgg16)\n",
    "K=5\n",
    "kf = sklearn.cross_validation.KFold(len(classes), n_folds=K)\n",
    "scores = []\n",
    "i=0\n",
    "for train, test in kf:\n",
    "    print(i,\"/\",K)\n",
    "    i+=1\n",
    "    model = sklearn.svm.SVC(C=100)#, C=1, gamma=0)\n",
    "    model.fit(imgfeatures[train, :], classes[train])\n",
    "    s=model.score(imgfeatures[test, :], classes[test])\n",
    "    print( s)\n",
    "    scores.append(s)\n",
    "print(numpy.mean(scores), numpy.std(scores))\n",
    "\n",
    "# vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=2)\n",
    "torch.save(vgg16.state_dict(), 'VGG16_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8d905fe-9f7f-484b-b926-9931ca887e34",
    "_uuid": "2803b24b7002a785833d300413e3bd8891f398f9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eval_model(vgg16, criterion)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "VGG16_v2-OCT2017_Retina.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
