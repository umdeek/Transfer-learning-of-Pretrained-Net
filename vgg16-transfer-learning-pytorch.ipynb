{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26359bdf-00a9-4280-9ec4-6ca7295148e4",
    "_uuid": "108609d6b29e75ec9f0fe15f66b4336f694c74b2"
   },
   "source": [
    "## VGG implementation with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4eb5ad5-03ba-4fc4-b417-451e30a01fd1",
    "_uuid": "be3002e45fc687fc33e7c7ae9d869ac7ee926b1a"
   },
   "source": [
    "*Python Modules*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "A lot of work here is derivative. Multiple sources have been referred to come up with the architecture and the solution given here though the task as a whole has not been directly used. I will make an effort to refer to the sources these to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0e05b5f8-fc51-404d-a9d2-5197aa283b73",
    "_uuid": "76fd0ec2a5eb7fbe49b51147eabbd109c61279c0",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sklearn.svm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "plt.ion() \n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ImageFolder loads the data directly from its path. transforms are used to then compose the same into the size needed for vggnet and alexnet. The data is then loaded based on the input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "e9d7ef88-fdbd-4b73-b64f-70294976d238",
    "_uuid": "83371f19c7f1e261bb0f9cc71f7a265c37a4c16a",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1525007461873,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "xvsy0IR4wheJ",
    "outputId": "bb02efaa-518c-4342-d6e5-7275a7d7fdd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7500 images under train\n",
      "Loaded 2500 images under test\n",
      "Classes: \n",
      "['beef_tartare', 'caesar_salad', 'chocolate_cake', 'croque_madame', 'escargots', 'fried_calamari', 'macaroni_and_cheese', 'poutine', 'spring_rolls', 'tuna_tartare']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"C:/Users/Umashanker Deekshith/Google Drive/Germany/Uni-Bonn/Semester 3/Deep Learning for VR/Exercise/DeepLearningWS/project/Deep-Learning-Project/src/images\"\n",
    "TRAIN = 'train'\n",
    "TEST = 'test'\n",
    "\n",
    "def data_loader(data_dir, TRAIN, TEST, image_crop_size = 224, mini_batch_size = 1 ):\n",
    "    # VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "    data_transforms = {\n",
    "        TRAIN: transforms.Compose([\n",
    "            # Data augmentation is a good practice for the train set\n",
    "            # Here, we randomly crop the image to 224x224 and\n",
    "            # randomly flip it horizontally. \n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        TEST: transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    image_datasets = {\n",
    "        x: datasets.ImageFolder(\n",
    "            os.path.join(data_dir, x), \n",
    "            transform=data_transforms[x]\n",
    "        )\n",
    "        for x in [TRAIN, TEST]\n",
    "    }\n",
    "\n",
    "    dataloaders = {\n",
    "        x: torch.utils.data.DataLoader(\n",
    "            image_datasets[x], batch_size=1,\n",
    "            shuffle=True, num_workers=1\n",
    "        )\n",
    "        for x in [TRAIN, TEST]\n",
    "    }\n",
    "    return dataloaders, image_datasets\n",
    "    \n",
    "dataloaders, image_datasets = data_loader(data_dir, TRAIN, TEST, image_crop_size = 224, mini_batch_size = 1 )\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, TEST]}\n",
    "\n",
    "for x in [TRAIN, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "classification_size = len(image_datasets[TRAIN].classes)\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ad1e66d-dcfc-429a-9664-e61b4d2944cc",
    "_uuid": "80417b6d4f2000be40245a090424b2452e1b78e1",
    "colab_type": "text",
    "id": "6f1fb14iwheO"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Some utility function to visualize the dataset and the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "fd84399d-83f9-4af7-8767-fe1d32856493",
    "_uuid": "48e7f7c02cab559638af532e98d371ebd8c89bfa",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1443,
     "status": "ok",
     "timestamp": 1525007467056,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "rphPgOQewheQ",
    "outputId": "0bec4c14-9968-4119-bb75-f896ddc21ebd"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders[TRAIN]))\n",
    "inputs_test, classes_test = next(iter(dataloaders[TEST]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "4be764f7-24ff-4611-8fc6-31b87e3ed171",
    "_uuid": "480b7181f00142865d3e971c799dd42bc9d1f7e5",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1003
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2623,
     "status": "ok",
     "timestamp": 1525007474565,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "SjHLMTldwheY",
    "outputId": "4c40caae-9d25-47e5-fe17-a4f3f944487e"
   },
   "outputs": [],
   "source": [
    "def set_up_network(net, freeze_training = True, clip_classifier = True, classification_size = 101):\n",
    "    if net == 'vgg16':\n",
    "    # Load the pretrained model from pytorch\n",
    "        network = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Freeze training for all layers\n",
    "        # Newly created modules have require_grad=True by default\n",
    "        if freeze_training:\n",
    "            for param in network.features.parameters():\n",
    "                param.require_grad = False\n",
    "\n",
    "        if clip_classifier:\n",
    "            features = list(network.classifier.children())[:-5] # Remove last layer\n",
    "            network.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "    \n",
    "    elif net == 'alexnet':\n",
    "        network = models.alexnet(pretrained=True)\n",
    "        if freeze_training:\n",
    "            for param in network.features.parameters():\n",
    "                param.require_grad = False\n",
    "        \n",
    "        if clip_classifier:\n",
    "            features = list(network.classifier.children())[:-4] # Remove last layer\n",
    "            network.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "    if classification_size != 1000 and clip_classifier == False:\n",
    "        num_features = network.classifier[6].in_features\n",
    "        features = list(network.classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features, classification_size)]) # Add our layer with 4 outputs\n",
    "        network.classifier = nn.Sequential(*features) # Replace the model cla\n",
    "#     print(network)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_nc = set_up_network('vgg16', freeze_training = True)\n",
    "alex_net_nc = set_up_network('alexnet', freeze_training = True)\n",
    "vgg16 = set_up_network('vgg16', freeze_training = False, clip_classifier = False, classification_size=classification_size)\n",
    "alex_net = set_up_network('alexnet', freeze_training = False, clip_classifier = False, classification_size = classification_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: For SVM on top of clipped VGG and AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(ipnet, train_batches = 10):\n",
    "    \n",
    "    imgfeatures = []\n",
    "    imglabels = []\n",
    "    for i, data in enumerate(dataloaders[TRAIN]):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "\n",
    "        # Use half training dataset\n",
    "        if i > train_batches:\n",
    "            break\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        feature = ipnet(inputs)\n",
    "        print(\"The shape of output is: \", feature.shape)\n",
    "        print(labels)\n",
    "        imgfeatures.append(feature.detach().numpy().flatten())\n",
    "        imglabels.append(labels.detach().numpy())\n",
    "        del inputs, labels, feature\n",
    "\n",
    "    return imgfeatures, imglabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_features_to_SVM(features, labels, train_batch_size, K=5 ):\n",
    "    print(\"The shape of the class is\", classes.shape)\n",
    "    kf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    kf.get_n_splits(features)\n",
    "    print(\"The split information is: \", kf)\n",
    "    scores = []\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    i=0\n",
    "    for train, test in kf.split(features):\n",
    "#     for train, test in kf:\n",
    "#         print(train)\n",
    "#         print(test)\n",
    "        print(i,\"/\",K)\n",
    "        i+=1\n",
    "        model = sklearn.svm.SVC(C=100)#, C=1, gamma=0)\n",
    "        model.fit(features[train, :], labels[train].ravel())\n",
    "        s=model.score(features[test, :], labels[test])\n",
    "        print(\"The score for this classification is: \", s)\n",
    "        scores.append(s)\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "def fit_features_to_SVM_new(features, labels, train_batch_size, K=5 ):\n",
    "    print(\"The shape of the class is\", classes.shape)\n",
    "    # split into a training and testing set\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    scores = []\n",
    "    for i in range(K):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=(1/K), random_state=42)\n",
    "#         print(\"The testing shape is: \", x_test.shape, y_test.shape)\n",
    "#         print(\"The training shape is: \", x_train.shape, y_train.shape)\n",
    "        model = sklearn.svm.SVC(C=100)#, C=1, gamma=0)\n",
    "        model.fit(x_train, y_train.ravel())\n",
    "        s=model.score(x_test, y_test)\n",
    "        print(\"The score for this classification is: \", s)\n",
    "        scores.append(s)\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 implementation with SVM as a classification layer. \n",
    "The batch size and other things can be classified from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "53eeb478-e106-49be-9682-0173e76640f8",
    "_uuid": "a0ddf54b1c45b7c61724cbe1e74ca0628f8b1d8e",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1525023518627,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "r1-I_IUUwhew",
    "outputId": "b764e48a-00fd-4eb5-f592-f4386a875ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 0/5.0The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([0])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([4])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([9])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([5])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([2])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([1])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([0])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([4])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([9])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([0])\n",
      "The shape of output is:  torch.Size([1, 4096])\n",
      "tensor([4])\n",
      "The shape of the class is torch.Size([1])\n",
      "The split information is:  KFold(n_splits=5, random_state=None, shuffle=False)\n",
      "(11, 4096)\n",
      "(11, 1)\n",
      "[ 3  4  5  6  7  8  9 10]\n",
      "[0 1 2]\n",
      "0 / 5\n",
      "The score for this classification is:  0.6666666666666666\n",
      "[ 0  1  2  5  6  7  8  9 10]\n",
      "[3 4]\n",
      "1 / 5\n",
      "The score for this classification is:  0.0\n",
      "[ 0  1  2  3  4  7  8  9 10]\n",
      "[5 6]\n",
      "2 / 5\n",
      "The score for this classification is:  0.5\n",
      "[ 0  1  2  3  4  5  6  9 10]\n",
      "[7 8]\n",
      "3 / 5\n",
      "The score for this classification is:  0.0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[ 9 10]\n",
      "4 / 5\n",
      "The score for this classification is:  0.0\n",
      "The mean and standard deviation of classification for vgg 16 is:  0.2333333333333333 0.29059326290271154\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 10\n",
    "\n",
    "imgfeatures_vgg, imglabels_vgg = get_features(vgg16_nc, train_batch_size)\n",
    "mean_accuracy, sd = fit_features_to_SVM(imgfeatures_vgg, imglabels_vgg, train_batch_size, K=5 )\n",
    "print(\"The mean and standard deviation of classification for vgg 16 is: \",mean_accuracy, sd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexnet implementation with SVM as a classification layer. \n",
    "The batch size and other things can be classified from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8d905fe-9f7f-484b-b926-9931ca887e34",
    "_uuid": "2803b24b7002a785833d300413e3bd8891f398f9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgfeatures_an, imglabels_an = get_features(alex_net_nc, train_batch_size)\n",
    "mean_accuracy, sd = fit_features_to_SVM(imgfeatures_an, imglabels_an, train_batch_size, K=5 )\n",
    "print(\"The mean and standard deviation of classification for alexnet is: \",mean_accuracy, sd)\n",
    "\n",
    "\n",
    "# vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=2)\n",
    "torch.save(vgg16.state_dict(), 'VGG16_v2-OCT_Retina_half_dataset.pt')\n",
    "torch.save(alex_net.state_dict(), 'ALEXNET_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: This one trains on top of the existing pre-trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Here, based on whether label smoothing is needed or not, a different loss function is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, gold, smoothing = False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(0)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "df23921b-f26e-496a-9a71-cdf2a9daa34e",
    "_uuid": "45d872d00fcfe93de8938b8741b4526af971c62b",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HTJWo25nwhef"
   },
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    vgg16.cuda() #.cuda() will move everything to the GPU side\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = cal_loss\n",
    "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dabed264-9a90-4888-ad3c-65924ad9c80d",
    "_uuid": "eff4aa2d97fd5443195ae2a2cd43c5f73e6775a3"
   },
   "source": [
    "## Training along with validation.\n",
    "Here, a split of 80% for training and 20% for validation is done for cross validation. It otherwise follows the standard training example given in pytorch site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "8f6936eb-ae6f-44ee-90a9-c7f817ba6eda",
    "_uuid": "abe5dc35b31e7971e7d63637866132f89e7d011d",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lHkiBU5fwhet"
   },
   "outputs": [],
   "source": [
    "def train_model(vgg, criterion, optimizer, scheduler, dataloaders, num_epochs=10, label_smoothing = False):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss_val = 0\n",
    "    avg_acc_val = 0\n",
    "    K = 5\n",
    "    train_batches = len(dataloaders[TRAIN])\n",
    "    train_bat = np.ones((train_batches, 1)) # This is a dummy variable as sklearn changed stuff and didn't do it right.\n",
    "    val_batches = 0.2*train_batches\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        \n",
    "        vgg.train(True)\n",
    "       \n",
    "        kf = sklearn.model_selection.KFold(n_splits=K)\n",
    "        kf.get_n_splits(train_bat)\n",
    "\n",
    "    \n",
    "        for train, test in kf.split(train_bat):\n",
    "        \n",
    "            for i, data in enumerate(dataloaders[TRAIN]):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
    "\n",
    "                # Use half training dataset\n",
    "#                 if i >= train_batches / 2:\n",
    "                if i >= 1:\n",
    "                    break\n",
    "                \n",
    "                if i not in train:\n",
    "                    continue\n",
    "                \n",
    "                inputs, labels = data\n",
    "\n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = vgg(inputs)\n",
    "\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_train += loss.item()\n",
    "#                 loss_train += loss.data[0]\n",
    "                acc_train += torch.sum(preds == labels.data)\n",
    "\n",
    "                del inputs, labels, outputs, preds\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Training complete\")\n",
    "            print()\n",
    "            # * 2 as we only used half of the dataset\n",
    "            avg_loss = loss_train * 2 / (dataset_sizes[TRAIN]*0.8)\n",
    "            avg_acc = acc_train * 2 / (dataset_sizes[TRAIN]*0.8)\n",
    "\n",
    "            vgg.train(False)\n",
    "            vgg.eval()\n",
    "\n",
    "            for i, data in enumerate(dataloaders[TRAIN]):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
    "\n",
    "                if i >= 1:\n",
    "                    break\n",
    "                if i not in test:\n",
    "                    continue\n",
    "                \n",
    "                inputs, labels = data\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = vgg(inputs)\n",
    "\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "#                 loss_val += loss.data[0]\n",
    "                loss_train += loss.item()\n",
    "                acc_val += torch.sum(preds == labels.data)\n",
    "\n",
    "                del inputs, labels, outputs, preds\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Testing complete\")\n",
    "\n",
    "            avg_loss_val = loss_val / (dataset_sizes[TRAIN]*0.2)\n",
    "            avg_acc_val = acc_val / (dataset_sizes[TRAIN]*0.2)\n",
    "\n",
    "            print()\n",
    "            print(\"Epoch {} result: \".format(epoch))\n",
    "            print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
    "            print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
    "            print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
    "            print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
    "            print('-' * 10)\n",
    "            print()\n",
    "\n",
    "            if avg_acc_val > best_acc:\n",
    "                best_acc = avg_acc_val\n",
    "                best_model_wts = copy.deepcopy(vgg.state_dict())\n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    vgg.load_state_dict(best_model_wts)\n",
    "    return vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "1b74302a-9418-4472-970f-76591d00cecb",
    "_uuid": "6d1b1cd49e177152940d8f667b9fa5e2e1c5e360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(vgg, criterion, smoothing_labels = False):\n",
    "    since = time.time()\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    loss_test = 0\n",
    "    acc_test = 0\n",
    "    \n",
    "    test_batches = len(dataloaders[TEST])\n",
    "    print(\"Evaluating model\")\n",
    "    print('-' * 10)\n",
    "    \n",
    "    for i, data in enumerate(dataloaders[TEST]):\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\rTest batch {}/{}\".format(i, test_batches), end='', flush=True)\n",
    "        if i >= 1:\n",
    "            break\n",
    "        vgg.train(False)\n",
    "        vgg.eval()\n",
    "        inputs, labels = data\n",
    "\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda(), requires_grad=True), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs, requires_grad=True), Variable(labels)\n",
    "\n",
    "        outputs = vgg(inputs)\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        loss = criterion(outputs, labels, smoothing=smoothing_labels)\n",
    "\n",
    "#         loss_test += loss.data[0]\n",
    "        loss_test += loss.item()\n",
    "\n",
    "        acc_test += torch.sum(preds == labels.data)\n",
    "\n",
    "        del inputs, labels, outputs, preds\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    avg_loss = loss_test / dataset_sizes[TEST]\n",
    "    avg_acc = acc_test / dataset_sizes[TEST]\n",
    "    \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n",
    "    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n",
      "Training batch 0/3750.0\n",
      "Validation batch 0/1500.0Testing complete\n",
      "\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0000\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0017\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0025\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0031\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0034\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "Training batch 0/3750.0\n",
      "Validation batch 0/1500.0Testing complete\n",
      "\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.0000\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.0012\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.0027\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.0034\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "Training batch 0/3750.0Training complete\n",
      "\n",
      "Validation batch 0/1500.0\n",
      "Epoch 1 result: \n",
      "Avg loss (train): 0.0037\n",
      "Avg acc (train): 0.0000\n",
      "Avg loss (val): 0.0000\n",
      "Avg acc (val): 0.0000\n",
      "----------\n",
      "\n",
      "\n",
      "Training completed in 0m 56s\n",
      "Best acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=2)\n",
    "torch.save(vgg16.state_dict(), 'VGG16_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "f7aec745-2087-4f60-987d-a34a995fd6a9",
    "_uuid": "7732406aae91a82348fcb830a5ff5785d22526fb",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1986,
     "status": "ok",
     "timestamp": 1525007488210,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "jSa-X3XVwheo",
    "outputId": "d6f338d4-5379-4fe8-aaec-cc8141b7cbb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the trained model\n",
      "Evaluating model\n",
      "----------\n",
      "Test batch 0/2500\n",
      "Evaluation completed in 0m 2s\n",
      "Avg loss (test): 0.0011\n",
      "Avg acc (test): 0.0000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing the trained model\")\n",
    "eval_model(vgg16, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = train_model(alex_net, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=2)\n",
    "torch.save(alex_net.state_dict(), 'AlexNet_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f7aec745-2087-4f60-987d-a34a995fd6a9",
    "_uuid": "7732406aae91a82348fcb830a5ff5785d22526fb",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1986,
     "status": "ok",
     "timestamp": 1525007488210,
     "user": {
      "displayName": "Carlo Alberto",
      "photoUrl": "//lh5.googleusercontent.com/-_sBZsyc315U/AAAAAAAAAAI/AAAAAAAAAgg/b4D9SE9jgD0/s50-c-k-no/photo.jpg",
      "userId": "107843268563316278814"
     },
     "user_tz": -120
    },
    "id": "jSa-X3XVwheo",
    "outputId": "d6f338d4-5379-4fe8-aaec-cc8141b7cbb5"
   },
   "outputs": [],
   "source": [
    "print(\"Testing the trained model\")\n",
    "eval_model(alex_net, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Using label smoothing regularisation\n",
    "The loss function is updated to include smoothing and is as shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=2, label_smoothing = True)\n",
    "torch.save(vgg16.state_dict(), 'VGG16_task3_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing the trained model\")\n",
    "eval_model(vgg16, criterion,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_net = train_model(alex_net, criterion, optimizer_ft, exp_lr_scheduler, dataloaders, num_epochs=2, label_smoothing = True)\n",
    "torch.save(alex_net.state_dict(), 'ALEXNet_Task3_v2-OCT_Retina_half_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing the trained model\")\n",
    "eval_model(alex_net, criterion,True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "VGG16_v2-OCT2017_Retina.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
